{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "NEXT_QNT = NEURONS = EPOCHS = 0\n",
    "with open(\"../Data/iniV2.pickle\", \"rb\") as f:\n",
    "    NEXT_QNT , NEURONS, EPOCHS = pickle.load(f)\n",
    "print(NEURONS,EPOCHS)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "64 100\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "with open(\"../Data/intents.json\",\"r\",encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for i,intent in enumerate(data[\"intents\"]):\n",
    "    intent[\"tag\"] = \"tag\"+str(i)\n",
    "\n",
    "with open(\"../Data/intents.json\",\"w\",encoding='utf-8') as file:\n",
    "    json.dump(data,file,indent=3,ensure_ascii=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        wrds = nlp(pattern.lower())\n",
    "        wrds = [token.lemma_ for token in wrds if not token.is_stop]\n",
    "        if wrds == []:\n",
    "            continue\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "\n",
    "    if intent[\"tag\"] not in labels:\n",
    "        labels.append(intent[\"tag\"])\n",
    "\n",
    "#from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "\n",
    "ponctuations = [\"?\",\"'\",'\"',\"!\",\".\",\",\"]\n",
    "\n",
    "words = [w for w in words if w not in ponctuations]\n",
    "#words = sorted(list(set(words)))\n",
    "#labels = sorted(labels)\n",
    "print(len(words))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1012\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "print(words[0:15])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['oi', 'eae', 'olá', 'dia', 'iai', 'thau', 'ver', 'falar', 'valer', 'util', 'ok', 'agradecer', 'ultimamente', 'tá', 'ir']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "    wrds = []\n",
    "    doc = nlp(' '.join(doc))\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.lemma_ not in ponctuations:\n",
    "            wrds.append(token.lemma_)\n",
    "    #wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "\n",
    "\n",
    "training = np.array(training)\n",
    "output = np.array(output)\n",
    "N_CLASSES = len(output[0])\n",
    "SHAPE = len(training[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "(len(training[0])+len(output[0]))/2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "592.0"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "#entradas+saídas/2 (opcional).\n",
    "def dense_layers(inputs):\n",
    "    x = tf.keras.layers.Dense(592,activation='relu')(inputs)\n",
    "    #x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    #x = tf.keras.layers.Dense(592,activation='relu')(x)\n",
    "    #x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    #x = tf.keras.layers.Dense(128,activation='relu')(x)\n",
    "    #x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    #x = tf.keras.layers.Dense(256,activation='relu')(x)\n",
    "    #x = tf.keras.layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def classfier_layer(x,N_CLASSES):\n",
    "    x = tf.keras.layers.Dense(N_CLASSES,activation='softmax',name='classification')(x)\n",
    "    return x\n",
    "\n",
    "def final_model(inputs,N_CLASSES):\n",
    "    dense = dense_layers(inputs)\n",
    "    \n",
    "    classfier = classfier_layer(dense,N_CLASSES)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs,outputs=classfier)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def define_and_compile_model(SHAPE,N_CLASSES):\n",
    "    inputs = tf.keras.layers.Input(shape=(SHAPE,))\n",
    "    \n",
    "    # create the model\n",
    "    model = final_model(inputs,N_CLASSES)\n",
    "    \n",
    "    # compile your model\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics = {'classification' : 'accuracy'})\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "model = define_and_compile_model(SHAPE,N_CLASSES)\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 1012)]            0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 592)               599696    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 592)               2368      \n",
      "_________________________________________________________________\n",
      "classification (Dense)       (None, 172)               101996    \n",
      "=================================================================\n",
      "Total params: 704,060\n",
      "Trainable params: 702,876\n",
      "Non-trainable params: 1,184\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "model.fit(training,output,epochs=50)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 7ms/step - loss: 0.7419 - accuracy: 0.0536\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6525 - accuracy: 0.4640\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5250 - accuracy: 0.6802\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3333 - accuracy: 0.7597\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2065 - accuracy: 0.7726\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.1151 - accuracy: 0.7671\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0636 - accuracy: 0.7523\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0405 - accuracy: 0.7616\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0302 - accuracy: 0.7542\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0266 - accuracy: 0.7745\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0236 - accuracy: 0.7856\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0205 - accuracy: 0.7837\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0184 - accuracy: 0.8096\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0168 - accuracy: 0.8152\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0155 - accuracy: 0.8429\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0143 - accuracy: 0.8410\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0130 - accuracy: 0.8447\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.8577\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.8688\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 0.8762\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.8725\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0088 - accuracy: 0.8780\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 0.8891\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0076 - accuracy: 0.8799\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0071 - accuracy: 0.8743\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0069 - accuracy: 0.8799\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0064 - accuracy: 0.8854\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0061 - accuracy: 0.8872\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0058 - accuracy: 0.8835\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 0.8854\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.8891\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 0.8762\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0048 - accuracy: 0.8909\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0047 - accuracy: 0.8835\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0046 - accuracy: 0.8799\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0045 - accuracy: 0.8872\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 0.8891\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0042 - accuracy: 0.8891\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0041 - accuracy: 0.8965\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0040 - accuracy: 0.8854\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0040 - accuracy: 0.8780\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0039 - accuracy: 0.8780\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0039 - accuracy: 0.8965\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0039 - accuracy: 0.8762\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0037 - accuracy: 0.8799\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0036 - accuracy: 0.8909\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 0.8835\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0036 - accuracy: 0.8872\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0036 - accuracy: 0.8872\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0035 - accuracy: 0.8780\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f599c53a730>"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "def bag_of_words(s):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    ponctuations = [\"?\",\"'\",'\"',\"!\",\".\",\",\"]\n",
    "\n",
    "    doc = nlp(s.lower())\n",
    "    s_words = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.lemma_ not in ponctuations:\n",
    "            s_words.append(token.lemma_)\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "    bag = np.array(bag)\n",
    "    return bag"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "while True:\n",
    "    inp = str(input(\"Digite: \"))\n",
    "    if inp == \"quit\":\n",
    "        break\n",
    "        \n",
    "    bag = bag_of_words(inp)\n",
    "    bag = bag.reshape((1,SHAPE))\n",
    "    predictions = model.predict(bag)\n",
    "    results_index = np.argmax(predictions)\n",
    "\n",
    "    tag = labels[results_index]\n",
    "\n",
    "    for tg in data['intents']:\n",
    "        if tg['tag'] == tag:\n",
    "            responses = tg['responses']\n",
    "            current_tag = tg\n",
    "\n",
    "    response = random.choice(responses)\n",
    "    confidence = predictions[0][results_index]\n",
    "    print(\"Model:\",response,confidence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: Oi como vai? 0.9888576\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "model.save(\"model.h6\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model.h6/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "with open(\"../Data/intents.json\",\"w\",encoding='utf-8') as file:\n",
    "    json.dump(data,file,indent=3,ensure_ascii=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "with open(\"../Data/dataV2.pickle\", \"wb\") as f:\n",
    "    pickle.dump((words, labels, training, output), f)    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "with open(\"../Data/iniV2.pickle\", \"wb\") as f:\n",
    "    pickle.dump((120, NEURONS, EPOCHS), f)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "f519070031a13e4af884df10703460b2ef730f6762a9d133e0aeaf8e55d09b65"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}