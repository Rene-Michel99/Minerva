{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "with open(\"../Data/intents.json\",\"r\",encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for i,intent in enumerate(data[\"intents\"]):\n",
    "    intent[\"tag\"] = \"tag\"+str(i)\n",
    "\n",
    "with open(\"../Data/intents.json\",\"w\",encoding='utf-8') as file:\n",
    "    json.dump(data,file,indent=3,ensure_ascii=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        wrds = nlp(pattern.lower())\n",
    "        wrds = [token.lemma_ for token in wrds if not token.is_stop]\n",
    "        if wrds == []:\n",
    "            continue\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "\n",
    "    if intent[\"tag\"] not in labels:\n",
    "        labels.append(intent[\"tag\"])\n",
    "\n",
    "#from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "\n",
    "ponctuations = [\"?\",\"'\",'\"',\"!\",\".\",\",\"]\n",
    "\n",
    "words = [w for w in words if w not in ponctuations]\n",
    "#words = sorted(list(set(words)))\n",
    "#labels = sorted(labels)\n",
    "print(len(words))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1012\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(words[0:15])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['oi', 'eae', 'olá', 'dia', 'iai', 'thau', 'ver', 'falar', 'valer', 'util', 'ok', 'agradecer', 'ultimamente', 'tá', 'ir']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "    wrds = []\n",
    "    doc = nlp(' '.join(doc))\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.lemma_ not in ponctuations:\n",
    "            wrds.append(token.lemma_)\n",
    "\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "\n",
    "\n",
    "training = np.array(training)\n",
    "output = np.array(output)\n",
    "N_CLASSES = len(output[0])\n",
    "SHAPE = len(training[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "(len(training[0])+len(output[0]))/2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "592.0"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "#entradas+saídas/2 (opcional).\n",
    "def dense_layers(inputs):\n",
    "    x = tf.keras.layers.Dense(592,activation='relu')(inputs)\n",
    "    \n",
    "    y = tf.keras.layers.Dense(592/2,activation='relu')(inputs)\n",
    "    y = tf.keras.layers.Dropout(0.2)(y)\n",
    "\n",
    "    concatted = tf.keras.layers.Concatenate()([x, y])\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(concatted)\n",
    "    #x = tf.keras.layers.Dense(128,activation='relu')(x)\n",
    "    #x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    #x = tf.keras.layers.Dense(256,activation='relu')(x)\n",
    "    #x = tf.keras.layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def classfier_layer(x,N_CLASSES):\n",
    "    x = tf.keras.layers.Dense(N_CLASSES,activation='softmax',name='classification')(x)\n",
    "    return x\n",
    "\n",
    "def final_model(inputs,N_CLASSES):\n",
    "    dense = dense_layers(inputs)\n",
    "    \n",
    "    classfier = classfier_layer(dense,N_CLASSES)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs,outputs=classfier)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def define_and_compile_model(SHAPE,N_CLASSES):\n",
    "    inputs = tf.keras.layers.Input(shape=(SHAPE,))\n",
    "    \n",
    "    # create the model\n",
    "    model = final_model(inputs,N_CLASSES)\n",
    "    \n",
    "    # compile your model\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics = {'classification' : 'accuracy'})\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "tf.reset_default_graph()\n",
    "model = define_and_compile_model(SHAPE,N_CLASSES)\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 1012)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 296)          299848      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 592)          599696      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 296)          0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 888)          0           dense_15[0][0]                   \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 888)          3552        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "classification (Dense)          (None, 172)          152908      batch_normalization_9[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,056,004\n",
      "Trainable params: 1,054,228\n",
      "Non-trainable params: 1,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "#%load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "from datetime import datetime\n",
    "\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.fit(training,output,epochs=50,callbacks=[tensorboard_callback])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 20ms/step - loss: 0.7569 - accuracy: 0.0684\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.6534 - accuracy: 0.4658\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.5021 - accuracy: 0.6802\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2961 - accuracy: 0.7431\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1649 - accuracy: 0.7708\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0887 - accuracy: 0.7652\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0467 - accuracy: 0.7560\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0332 - accuracy: 0.7671\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0255 - accuracy: 0.7763\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0217 - accuracy: 0.7930\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0189 - accuracy: 0.8096\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0169 - accuracy: 0.8041\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0153 - accuracy: 0.8170\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0139 - accuracy: 0.8355\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.0130 - accuracy: 0.8429\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0117 - accuracy: 0.8466\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0107 - accuracy: 0.8577\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0097 - accuracy: 0.8725\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0091 - accuracy: 0.8725\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0084 - accuracy: 0.8595\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0077 - accuracy: 0.8743\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0073 - accuracy: 0.8743\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0069 - accuracy: 0.8688\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0064 - accuracy: 0.8762\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0060 - accuracy: 0.8780\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0058 - accuracy: 0.8762\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0055 - accuracy: 0.8965\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0053 - accuracy: 0.8799\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0051 - accuracy: 0.8854\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0049 - accuracy: 0.8817\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0049 - accuracy: 0.8835\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0046 - accuracy: 0.8817\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0044 - accuracy: 0.8872\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0044 - accuracy: 0.8891\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0043 - accuracy: 0.8965\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0042 - accuracy: 0.8835\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0041 - accuracy: 0.8946\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0040 - accuracy: 0.8817\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0040 - accuracy: 0.8835\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0039 - accuracy: 0.8928\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0037 - accuracy: 0.8891\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0038 - accuracy: 0.8743\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0036 - accuracy: 0.8891\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0037 - accuracy: 0.8706\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0037 - accuracy: 0.8799\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0036 - accuracy: 0.8762\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0036 - accuracy: 0.8854\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.0036 - accuracy: 0.8854\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0034 - accuracy: 0.8909\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.0035 - accuracy: 0.8891\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d5d98d1f0>"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Executar apenas uma vez\n",
    "%tensorboard --logdir logs/fit;"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "def bag_of_words(s):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    ponctuations = [\"?\",\"'\",'\"',\"!\",\".\",\",\"]\n",
    "\n",
    "    doc = nlp(s.lower())\n",
    "    s_words = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.lemma_ not in ponctuations:\n",
    "            s_words.append(token.lemma_)\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "    bag = np.array(bag)\n",
    "    return bag"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "while True:\n",
    "    inp = str(input(\"Digite: \"))\n",
    "    if inp == \"quit\":\n",
    "        break\n",
    "        \n",
    "    bag = bag_of_words(inp)\n",
    "    bag = bag.reshape((1,SHAPE))\n",
    "    predictions = model.predict(bag)\n",
    "    results_index = np.argmax(predictions)\n",
    "\n",
    "    tag = labels[results_index]\n",
    "\n",
    "    for tg in data['intents']:\n",
    "        if tg['tag'] == tag:\n",
    "            responses = tg['responses']\n",
    "            current_tag = tg\n",
    "\n",
    "    response = random.choice(responses)\n",
    "    confidence = predictions[0][results_index]\n",
    "    print(\"Model:\",response,confidence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: Olá como vai? 0.9939075\n",
      "Model: Tenho notícias, <news> 0.076253094\n",
      "Model: <date> 0.9996803\n",
      "Model: Fico feliz em saber que se sente bem 0.8009597\n",
      "Model: Vou bem, e você? 0.3914616\n",
      "Model: Estou bem, e você? 0.3914616\n",
      "Model: <finalizar> 0.99730563\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model.save(\"model_2.h5\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: model.h6/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "with open(\"../Data/intents.json\",\"w\",encoding='utf-8') as file:\n",
    "    json.dump(data,file,indent=3,ensure_ascii=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "with open(\"../Data/dataV2.pickle\", \"wb\") as f:\n",
    "    pickle.dump((words, labels, training, output), f)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "f519070031a13e4af884df10703460b2ef730f6762a9d133e0aeaf8e55d09b65"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}