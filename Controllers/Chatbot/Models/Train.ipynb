{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "with open(\"../Data/intents.json\",\"r\",encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for i,intent in enumerate(data[\"intents\"]):\n",
    "    intent[\"tag\"] = \"tag\"+str(i)\n",
    "\n",
    "with open(\"../Data/intents.json\",\"w\",encoding='utf-8') as file:\n",
    "    json.dump(data,file,indent=3,ensure_ascii=False)\n",
    "print(len(data[\"intents\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "156\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        wrds = nlp(pattern.lower())\n",
    "        wrds = [token.lemma_ for token in wrds if not token.is_stop]\n",
    "        if wrds == []:\n",
    "            continue\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "\n",
    "    if intent[\"tag\"] not in labels:\n",
    "        labels.append(intent[\"tag\"])\n",
    "\n",
    "#from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "\n",
    "ponctuations = [\"?\",\"'\",'\"',\"!\",\".\",\",\"]\n",
    "\n",
    "words = [w for w in words if w not in ponctuations]\n",
    "#words = sorted(list(set(words)))\n",
    "#labels = sorted(labels)\n",
    "print(len(words))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "891\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(words[20:35])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['mau', 'sentir', 'mau', 'sentir', 'tô', 'mau', 'gostar', 'gostar', 'gostar', 'algum', 'gostar', 'nome', 'chamar', 'nome', 'chamar']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "    wrds = []\n",
    "    doc = nlp(' '.join(doc))\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.lemma_ not in ponctuations:\n",
    "            wrds.append(token.lemma_)\n",
    "\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    bag = np.array(bag)\n",
    "    bag = bag.reshape((1,bag.shape[0],1))\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "\n",
    "\n",
    "training = np.array(training)\n",
    "output = np.array(output)\n",
    "N_CLASSES = len(output[0])\n",
    "SHAPE = training[0].shape[1]\n",
    "print(SHAPE,training[0].shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "891 (1, 891, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "(len(training[0])+len(output[0]))/2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "523.5"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "#entradas+saídas/2 (opcional).\n",
    "def dense_layers(inputs):\n",
    "    x = tf.keras.layers.Conv2D(2,3,activation='relu',input_shape=(1,SHAPE,1),padding='same')(inputs)\n",
    "    x = tf.keras.layers.Conv2D(2,3,activation='relu',padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(2,3,activation='relu',padding='same')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(520,activation='relu')(x)\n",
    "    \n",
    "    y = tf.keras.layers.Dense(520/2,activation='relu')(x)\n",
    "    y = tf.keras.layers.Dropout(0.2)(y)\n",
    "\n",
    "    concatted = tf.keras.layers.Concatenate()([x, y])\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(concatted)\n",
    "    return x\n",
    "\n",
    "def classfier_layer(x,N_CLASSES):\n",
    "    x = tf.keras.layers.Dense(N_CLASSES,activation='softmax',name='classification')(x)\n",
    "    return x\n",
    "\n",
    "def final_model(inputs,N_CLASSES):\n",
    "    dense = dense_layers(inputs)\n",
    "    \n",
    "    classfier = classfier_layer(dense,N_CLASSES)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs,outputs=classfier)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def define_and_compile_model(SHAPE,N_CLASSES):\n",
    "    inputs = tf.keras.layers.Input(shape=(1,SHAPE,1))\n",
    "    \n",
    "    # create the model\n",
    "    model = final_model(inputs,N_CLASSES)\n",
    "    \n",
    "    # compile your model\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics = {'classification' : 'accuracy'})\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "model = define_and_compile_model(SHAPE,N_CLASSES)\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 1, 891, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 1, 891, 2)    20          input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 1, 891, 2)    38          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 1, 891, 2)    38          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1782)         0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 520)          927160      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 260)          135460      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 260)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 780)          0           dense_4[0][0]                    \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 780)          3120        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "classification (Dense)          (None, 156)          121836      batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,187,672\n",
      "Trainable params: 1,186,112\n",
      "Non-trainable params: 1,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "!rm -rf ./logs/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "#%load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "from datetime import datetime\n",
    "\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.fit(training,output,epochs=20,callbacks=[tensorboard_callback])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.0035 - accuracy: 0.8884\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.0035 - accuracy: 0.8947\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0034 - accuracy: 0.8863\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.0033 - accuracy: 0.8926\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.0035 - accuracy: 0.8821\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0033 - accuracy: 0.8968\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.0035 - accuracy: 0.8758\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.0033 - accuracy: 0.8926\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.0034 - accuracy: 0.8968\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.0033 - accuracy: 0.8905\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0033 - accuracy: 0.8905\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.0031 - accuracy: 0.8905\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.0032 - accuracy: 0.8947\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.0032 - accuracy: 0.8926\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0031 - accuracy: 0.8905\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.0031 - accuracy: 0.9032\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.0032 - accuracy: 0.8842\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.0030 - accuracy: 0.8926\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.0031 - accuracy: 0.8821\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0031 - accuracy: 0.8926\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1b1e7679d0>"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# Executar apenas uma vez\n",
    "# http://localhost:6006/\n",
    "%tensorboard --logdir './logs/fit'"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c99305fc6dbdd8f9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c99305fc6dbdd8f9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "def bag_of_words(s):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    ponctuations = [\"?\",\"'\",'\"',\"!\",\".\",\",\"]\n",
    "\n",
    "    doc = nlp(s.lower())\n",
    "    s_words = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.lemma_ not in ponctuations:\n",
    "            s_words.append(token.lemma_)\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "    bag = np.array(bag)\n",
    "    return bag"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "while True:\n",
    "    inp = str(input(\"Digite: \"))\n",
    "    if inp == \"quit\":\n",
    "        break\n",
    "        \n",
    "    bag = bag_of_words(inp)\n",
    "    bag = bag.reshape((1,SHAPE))\n",
    "    predictions = model.predict(bag)\n",
    "    results_index = np.argmax(predictions)\n",
    "\n",
    "    tag = labels[results_index]\n",
    "\n",
    "    for tg in data['intents']:\n",
    "        if tg['tag'] == tag:\n",
    "            responses = tg['responses']\n",
    "            current_tag = tg\n",
    "\n",
    "    response = random.choice(responses)\n",
    "    confidence = predictions[0][results_index]\n",
    "    print(\"Model:\",response,confidence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: Olá como vai? 0.9234132\n",
      "Model: Que bom 0.6732022\n",
      "Model: Vou bem, e você? 0.1849314\n",
      "Model: Vou bem, e você? 0.1849314\n",
      "Model: Fico feliz em saber que se sente bem 0.6732022\n",
      "Model: Vou bem, e você? 0.1849314\n",
      "Model: <status> 0.1849314\n",
      "Model: Te vejo depois 0.87662673\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "model.save(\"model_2.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "with open(\"../Data/intents.json\",\"w\",encoding='utf-8') as file:\n",
    "    json.dump(data,file,indent=3,ensure_ascii=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "with open(\"../Data/dataV2.pickle\", \"wb\") as f:\n",
    "    pickle.dump((words, labels, training, output), f)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "f519070031a13e4af884df10703460b2ef730f6762a9d133e0aeaf8e55d09b65"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}